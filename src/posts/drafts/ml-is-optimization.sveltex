---
title: "Machine Learning Isn't Magic; It's Just Optimization"
date: "2026-01-30"
description: "Machine learning demystified: at its core, it's just finding the minimum of a function."
tags: ["machine-learning", "optimization"]
draft: true
---

Rather than cataloging machine learning models and techniques, I want to start from first principles and build up the idea of machine learning.
Some problems seem intractable in computer science, like programmatically identifying if an image shows a dog or not.
If you knew nothing about machine learning, it's hard to imagine where you'd even start.

On the other hand, there are far simpler problems that we do know how to solve.
So it makes sense to start with a simple problem we can solve and gradually make it harder until it resembles the difficult problem we want to solve.

## Optimizing a Quadratic Polynomial

Say you're running a business and want to maximize your revenue selling a product.
You notice a relationship where if you price the product at $p$ dollars, you can sell $100 - 2p$ units.
Now you want to find the value of $p$ that maximizes your revenue $R(p) = p(100 - 2p) = -2p^2 + 100p$.

![R(p) = p(100 − 2p)](/images/posts/ml-is-optimization/quadratic-revenue.png)

There are multiple approaches you could use to maximize a simple quadratic polynomial.
One way is to complete the square and rewrite $R(p)$ as $-2(p - 25)^2 + 1250$ which is maximized at $R(25) = 1250$ because the first term is a negative squared value and is always nonpositive.

A more general purpose approach is to find the critical points, which you get when you set the derivative to $0$, and then find the critical point that maximizes the function.
In this case, $R'(p) = -4p + 100 = 0 \implies p = 25$ is the only critical point.

An even more general purpose approach for when you can't solve $R'(p)=0$ is to start at a random point and iteratively move in the direction that increases the function.
Recall the limit definition of the derivative: $R'(p) = \lim_{h \to 0} \frac{R(p + h) - R(p)}{h}$.
Rearranging, this tells us that for a small step $h$, the function changes by approximately $R(p + h) - R(p) \approx h \cdot R'(p)$.
So if $R'(p) > 0$, a positive step increases $R$, and if $R'(p) < 0$, a negative step increases $R$.

The simplest approach is to just use the sign of the derivative: $p_{n+1} = p_n + \alpha \cdot \text{sign}(R'(p_n))$, where $\alpha$ is a fixed step size.
Starting at $p_0 = 5$ with $\alpha = 0.1$:

$$
\begin{aligned}
p_0 &= 5.00 & \text{sign}(R'(p_0)) &= +1 & R(p_0) &= 450.00 \\
p_1 &= 5 + 0.1 \cdot (+1) = 5.10 & \text{sign}(R'(p_1)) &= +1 & R(p_1) &= 457.98 \\
p_2 &= 5.10 + 0.1 \cdot (+1) = 5.20 & \text{sign}(R'(p_2)) &= +1 & R(p_2) &= 465.92 \\
p_3 &= 5.20 + 0.1 \cdot (+1) = 5.30 & \text{sign}(R'(p_3)) &= +1 & R(p_3) &= 473.82 \\
p_4 &= 5.30 + 0.1 \cdot (+1) = 5.40 & \text{sign}(R'(p_4)) &= +1 & R(p_4) &= 481.68 \\
\vdots & & & & \vdots \\
p_{200} &= 25.00 & \text{sign}(R'(p_{200})) &= 0 & R(p_{200}) &= 1250.00
\end{aligned}
$$

Each step moves $p$ by a fixed amount in the direction of increasing $R$. With $\alpha = 0.1$, we land exactly on $p = 25$ after $200$ steps; in general you'd stop when the derivative is near zero or the sign flips.

![Sign-based ascent on R(p)](/images/posts/ml-is-optimization/quadratic-sign-ascent.png)

This works, but the fixed step size is a limitation. If the step is too large, we might overshoot the maximum and oscillate around it. If it's too small, convergence is slow. We can do better by also using the *magnitude* of the derivative, taking larger steps when we're far from the peak (where the slope is steep) and smaller steps as we approach it (where the slope flattens).

This gives us gradient ascent with the update rule $p_{n+1} = p_n + \alpha R'(p_n)$, where $\alpha$ is a step size.
Starting at $p_0 = 5$ with $\alpha = 0.1$:

$$
\begin{aligned}
p_0 &= 5.00 & R'(p_0) &= 80.00 & R(p_0) &= 450.00 \\
p_1 &= 5 + 0.1 \cdot 80 = 13.00 & R'(p_1) &= 48.00 & R(p_1) &= 962.00 \\
p_2 &= 13 + 0.1 \cdot 48 = 17.80 & R'(p_2) &= 28.80 & R(p_2) &= 1146.32 \\
p_3 &= 17.80 + 0.1 \cdot 28.8 = 20.68 & R'(p_3) &= 17.28 & R(p_3) &= 1212.68 \\
p_4 &= 20.68 + 0.1 \cdot 17.28 = 22.41 & R'(p_4) &= 10.37 & R(p_4) &= 1236.56 \\
p_5 &= 22.41 + 0.1 \cdot 10.37 = 23.44 & R'(p_5) &= 6.22 & R(p_5) &= 1245.16
\end{aligned}
$$

Now the steps naturally get smaller as we approach the peak, allowing for both fast initial progress and precise convergence.

![Gradient ascent on R(p)](/images/posts/ml-is-optimization/quadratic-gradient-ascent.png)

Now let's move on to a more complex function.

## Optimizing a Single Variable Function

The previous example was a quadratic, where we could solve $R'(p) = 0$ analytically to find the critical point. But what if the derivative equation has no closed-form solution?

Consider $f(x) = x^4 - x^3 - x^2 + e^{-x^2}$. Setting the derivative to zero gives $4x^3 - 3x^2 - 2x - 2xe^{-x^2} = 0$, which we can't solve algebraically. This is where gradient descent becomes essential rather than just convenient.

![f(x) = x^4 - x^3 - x^2 + e^(-x^2)](/images/posts/ml-is-optimization/single-var-function.png)

This function has two local minima, one near $x = -0.62$ and a deeper one near $x = 1.24$. Since we're minimizing instead of maximizing, the update rule becomes $x_{n+1} = x_n - \alpha f'(x_n)$ (note the minus sign). Starting from $x_0 = 2.0$ with $\alpha = 0.1$:

$$
\begin{aligned}
x_0 &= 2.00 & f'(x_0) &= 15.93 & f(x_0) &= 4.02 \\
x_1 &= 2.00 - 0.1 \cdot 15.93 = 0.41 & f'(x_1) &= -1.73 & f(x_1) &= 0.64 \\
x_2 &= 0.41 - 0.1 \cdot (-1.73) = 0.58 & f'(x_2) &= -2.22 & f(x_2) &= 0.29 \\
x_3 &= 0.58 - 0.1 \cdot (-2.22) = 0.80 & f'(x_3) &= -2.31 & f(x_3) &= -0.22 \\
x_4 &= 0.80 - 0.1 \cdot (-2.31) = 1.03 & f'(x_4) &= -1.57 & f(x_4) &= -0.69
\end{aligned}
$$

After a few more steps, $x$ converges to the minimum at $x \approx 1.24$.

![Gradient descent from two starting points](/images/posts/ml-is-optimization/single-var-gradient-descent.png)

The plot shows two different starting points. Starting from $x_0 = 2.0$ leads to the deeper minimum on the right, while starting from $x_0 = -1.2$ leads to the shallower minimum on the left. The starting point determines which minimum we find, since gradient descent finds a local minimum, not necessarily the global one.

## Optimizing a Two Variable Function

Now let's add another variable. Consider $f(x, y) = x^2 + 3y^2 - xy$, which has a minimum at the origin.

![f(x, y) = x² + 3y² - xy](/images/posts/ml-is-optimization/two-var-function.png)

In the single variable case, we minimized $f(x)$ by changing $x$ in the direction that decreased $f$. With two variables, we can ask the same question twice: how should we change $x$ (holding $y$ fixed) to decrease $f$, and how should we change $y$ (holding $x$ fixed) to decrease $f$? A small step $(\Delta x, \Delta y)$ changes the function by the sum of the partial contributions:

$$
\Delta f \approx \frac{\partial f}{\partial x}\Delta x + \frac{\partial f}{\partial y}\Delta y
$$
Those two partial derivatives form a vector called the gradient:

$$
\nabla f = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right) = (2x - y, \; 6y - x)
$$

The gradient points in the direction of increasing $f$, so for small enough $\alpha$ we subtract it to descend. This is the same idea as the single variable update $x_{n+1} = x_n - \alpha f'(x_n)$, where we just replace the scalar derivative with a vector and take a step proportional to it in parameter space. The update rule is structurally identical to before:

$$
\mathbf{p} = (x, y), \qquad
\mathbf{p}_{n+1} = \mathbf{p}_n - \alpha \nabla f(\mathbf{p}_n)
$$

Starting from $(x_0, y_0) = (3, 2)$ with $\alpha = 0.1$:

$$
\begin{aligned}
(x_0, y_0) &= (3.00, 2.00) & \nabla f &= (4.00, 9.00) & f &= 15.00 \\
(x_1, y_1) &= (2.60, 1.10) & \nabla f &= (4.10, 4.00) & f &= 7.53 \\
(x_2, y_2) &= (2.19, 0.70) & \nabla f &= (3.68, 2.01) & f &= 4.73 \\
(x_3, y_3) &= (1.82, 0.50) & \nabla f &= (3.14, 1.18) & f &= 3.15 \\
(x_4, y_4) &= (1.51, 0.38) & \nabla f &= (2.64, 0.77) & f &= 2.14
\end{aligned}
$$

![Gradient descent on f(x, y)](/images/posts/ml-is-optimization/two-var-gradient-descent.png)

Notice the path doesn't head straight toward the minimum, it curves because the function is steeper in the $y$ direction. But the algorithm doesn't need to know this; it just follows the gradient downhill at each step.

Nothing fundamentally changed from the single variable case. We're still computing a derivative and stepping opposite to it. The only difference is that our derivative is now a vector.
Note that unlike the single variable case where you either have the choice to increase or decrease the variable and only one direction can decrease the function locally, the two variable case has many directions.

## Creating a Simple Model

This is where we stop going over introductory calculus concepts and shift the focus to machine learning.

<!-- TODO: bridge from optimization to ML. Start with a tiny dataset and a
     model (e.g. linear regression with slope/intercept). Define error as a
     function of the model parameters (this is the "loss"). Show that the loss
     is just another function to minimize, exactly like the ones above.
     The image classification example (function that takes an image and says
     if it contains a dog or not) may be too big a jump here. Consider
     starting with something simpler and mentioning images as where this
     scales to. -->

## Creating a Slightly Better Model

<!-- TODO: iterate on the simple model. Maybe go from a line to a curve, or
     add a feature. Show that "better model" = "different function family to
     optimize over." The optimization machinery stays the same. -->
