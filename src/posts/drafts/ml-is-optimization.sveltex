---
title: "Machine Learning Isn't Magic; It's Just Optimization"
date: "2026-01-30"
description: "Machine learning demystified: at its core, it's just finding the minimum of a function."
tags: ["machine-learning", "optimization"]
draft: true
---

Instead of looking at machine learning models and techniques, I want to go through the first principles of what machine learning aims to do. Some problems seem intractable — like programmatically identifying the objects in an image. If you knew nothing about machine learning, it's hard to imagine where you'd even start.

On the other hand, there are far simpler problems in math that we do know how to solve. So it makes sense to start with one of those and gradually reframe it into the intractable problem we actually want to solve. A common math problem is minimizing or maximizing a function, so let's begin there with a simple quadratic polynomial.

## Optimizing a Quadratic Polynomial

Say you're running a business and want to maximize your revenue selling a product. You establish that if you price the product at p dollars, you can sell 100 - 2p units. Now you want to find the value of p that maximizes your revenue.

<!-- TODO: revenue function R(p) = p(100 - 2p) = 100p - 2p^2 -->

There are multiple ways to maximize this. You could plot points and draw the curve. You could also complete the square to rewrite the function.

<!-- TODO: completed square form showing R(p) = -2(p - 25)^2 + 1250 -->

Since the squared term is always non-negative, the negated squared term is always non-positive — so the whole expression is at most 1250, achieved when p = 25. Or you could use calculus and set the derivative to 0 to find the critical points.

<!-- TODO: derivative R'(p) = 100 - 4p = 0, so p = 25 -->

These approaches work fine when you know the function and it's simple enough to solve analytically. Things get harder when the function is more complex, has many variables, or when you don't even know the function upfront. Much of machine learning boils down to these core ideas.

## Optimizing a Single Variable Function

<!-- TODO: generalize beyond quadratics to an arbitrary f(x) where analytic
     solutions aren't available. Introduce gradient descent here:
     x_{n+1} = x_n - α f'(x_n). Walk through steps numerically on a simple
     example. Show different learning rates (too small, too large, just right).
     This is the key bridge from "calculus we know" to "iterative methods ML
     actually uses." -->

## Optimizing a Two Variable Function

<!-- TODO: extend to f(x, y). Gradient is now a vector, update rule is the
     same. Contour plot with descent path. Key point: nothing conceptually
     changed, just more parameters. "Now imagine a million parameters." -->

## Creating a Simple Model

This is where we stop going over introductory calculus concepts and shift the focus to machine learning.

<!-- TODO: bridge from optimization to ML. Start with a tiny dataset and a
     model (e.g. linear regression with slope/intercept). Define error as a
     function of the model parameters — this is the "loss." Show that the loss
     is just another function to minimize, exactly like the ones above.
     The image classification example (function that takes an image and says
     if it contains a dog or not) may be too big a jump here — consider
     starting with something simpler and mentioning images as where this
     scales to. -->

## Creating a Slightly Better Model

<!-- TODO: iterate on the simple model. Maybe go from a line to a curve, or
     add a feature. Show that "better model" = "different function family to
     optimize over." The optimization machinery stays the same. -->
