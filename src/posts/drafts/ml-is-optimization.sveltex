---
title: "Machine Learning Isn't Magic; It's Just Optimization"
date: "2026-01-30"
description: "Machine learning demystified: at its core, it's just finding the minimum of a function."
tags: ["machine-learning", "optimization"]
draft: true
---

Instead of looking at machine learning models and techniques, I want to go through the first principles of what machine learning aims to do. There are a lot of problems that were seemingly intractable like programmatically identifying the objects in an image. If you knew nothing about machine learning, it seems like an impossible problem to solve. On the other hand, there are far simpler problems in math that we do know how to solve, so naturally it makes sense to start off with a simple math problem we can solve and gradually reframe the problem to the intractable problem we wanted to solve. A common math problem we often want to solve is minimizing or maximizing a function, so let's start off with minimizing or maximizing a simple quadratic polynomial.

## Optimizing a Quadratic Polynomial

An example of where this may come up is when running a business, where you want to maximize your revenue selling a product, and you establish a relationship where if you price the product at p dollars, then the number of units of the product you can sell is 100 - 2p. This leads you to wanting to identify the value of p that maximizes your revenue.

<!-- TODO: revenue function R(p) = p(100 - 2p) = 100p - 2p^2 -->

There are multiple ways to try to maximize this. You could perhaps plot points and draw what this curve looks like. You could also try completing the square to rewrite the function.

<!-- TODO: completed square form showing R(p) = -2(p - 25)^2 + 1250 -->

It's pretty clear this function is maximized at p = 25 since the quadratic term has a maximum value of 0 due to the negative coefficient. You could use calculus and set the derivative to 0 to find the critical points.

<!-- TODO: derivative R'(p) = 100 - 4p = 0, so p = 25 -->

These approaches work fine when you know what function you want to minimize or maximize, and the function is simple. It starts getting more complicated when you have more complex functions, the functions have multiple variables, and when you don't know what function you want to minimize or maximize. Much of machine learning boils down to these core ideas.

## Optimizing a Single Variable Function

<!-- TODO: generalize beyond quadratics to an arbitrary f(x) where analytic
     solutions aren't available. Introduce gradient descent here:
     x_{n+1} = x_n - α f'(x_n). Walk through steps numerically on a simple
     example. Show different learning rates (too small, too large, just right).
     This is the key bridge from "calculus we know" to "iterative methods ML
     actually uses." -->

## Optimizing a Two Variable Function

<!-- TODO: extend to f(x, y). Gradient is now a vector, update rule is the
     same. Contour plot with descent path. Key point: nothing conceptually
     changed, just more parameters. "Now imagine a million parameters." -->

## Creating a Simple Model

This is where we stop going over introductory calculus concepts and shift the focus to machine learning.

<!-- TODO: bridge from optimization to ML. Start with a tiny dataset and a
     model (e.g. linear regression with slope/intercept). Define error as a
     function of the model parameters — this is the "loss." Show that the loss
     is just another function to minimize, exactly like the ones above.
     The image classification example (function that takes an image and says
     if it contains a dog or not) may be too big a jump here — consider
     starting with something simpler and mentioning images as where this
     scales to. -->

## Creating a Slightly Better Model

<!-- TODO: iterate on the simple model. Maybe go from a line to a curve, or
     add a feature. Show that "better model" = "different function family to
     optimize over." The optimization machinery stays the same. -->
