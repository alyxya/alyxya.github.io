---
title: "Machine Learning Isn't Magic; It's Just Optimization"
date: "2026-01-30"
description: "Machine learning demystified: at its core, it's just finding the minimum of a function."
tags: ["machine-learning", "optimization"]
draft: true
---

Rather than cataloging machine learning models and techniques, I want to start from first principles and build up the idea of machine learning.
Some problems seem intractable in computer science, like programmatically identifying whether an image shows a dog.
If you knew nothing about machine learning, it's hard to imagine where you'd even start.

On the other hand, there are far simpler problems we do know how to solve.
So it makes sense to start with a simple problem we can solve and gradually make it harder until it resembles the difficult problem we care about.

## Optimizing a Quadratic Polynomial

Suppose we're running a business and want to maximize our revenue from a product.
We notice a relationship, where if we price the product at $p$ dollars, we can sell $100 - 2p$ units.
Now we want to find the value of $p$ that maximizes our revenue $R(p) = p(100 - 2p) = -2p^2 + 100p$.

![R(p) = p(100 − 2p)](/images/posts/ml-is-optimization/quadratic-revenue.png)

There are multiple approaches we could use to maximize a simple quadratic polynomial.
One way is to complete the square and rewrite $R(p)$ as $-2(p - 25)^2 + 1250$, which is maximized at $R(25) = 1250$ since the squared term with a negative coefficient is always nonpositive.

A more general purpose approach is to find the critical points by setting the derivative to $0$, then choose the one that maximizes the function.
In this case, $R'(p) = -4p + 100 = 0 \implies p = 25$ is the only critical point.

A still more general approach, when we can't solve $R'(p)=0$, is to start at a random point and iteratively move in the direction that increases the function.
Recall the limit definition of the derivative: $R'(p) = \lim_{h \to 0} \frac{R(p + h) - R(p)}{h}$.
Rearranging, this tells us that for a small step $h$, the function changes by approximately $R(p + h) - R(p) \approx h \cdot R'(p)$.
So if $R'(p) > 0$, a positive step increases $R$, and if $R'(p) < 0$, a negative step increases $R$.

The simplest approach is to just use the sign of the derivative: $p_{n+1} = p_n + \alpha \cdot \text{sign}(R'(p_n))$, where $\alpha$ is a fixed step size.
Starting at $p_0 = 5$ with $\alpha = 0.1$:

$$
\begin{aligned}
p_0 &= 5.00 & \text{sign}(R'(p_0)) &= +1 & R(p_0) &= 450.00 \\
p_1 &= 5 + 0.1 \cdot (+1) = 5.10 & \text{sign}(R'(p_1)) &= +1 & R(p_1) &= 457.98 \\
p_2 &= 5.10 + 0.1 \cdot (+1) = 5.20 & \text{sign}(R'(p_2)) &= +1 & R(p_2) &= 465.92 \\
p_3 &= 5.20 + 0.1 \cdot (+1) = 5.30 & \text{sign}(R'(p_3)) &= +1 & R(p_3) &= 473.82 \\
p_4 &= 5.30 + 0.1 \cdot (+1) = 5.40 & \text{sign}(R'(p_4)) &= +1 & R(p_4) &= 481.68 \\
\vdots & & & & \vdots \\
p_{200} &= 25.00 & \text{sign}(R'(p_{200})) &= 0 & R(p_{200}) &= 1250.00
\end{aligned}
$$

Each step moves $p$ by a fixed amount in the direction of increasing $R$. With $\alpha = 0.1$, we land exactly on $p = 25$ after $200$ steps; in general we'd stop when the derivative is near zero or the sign flips.

![Sign-based ascent on R(p)](/images/posts/ml-is-optimization/quadratic-sign-ascent.png)

This works, but the fixed step size is a limitation. If the step is too large, we might overshoot the maximum and oscillate around it. If it's too small, convergence is slow. We can do better by also using the *magnitude* of the derivative, taking larger steps when we're far from the peak (where the slope is steep) and smaller steps as we approach it (where the slope flattens).

This gives us gradient ascent with the update rule $p_{n+1} = p_n + \alpha R'(p_n)$, where $\alpha$ controls the step size.
Starting at $p_0 = 5$ with $\alpha = 0.1$:

$$
\begin{aligned}
p_0 &= 5.00 & R'(p_0) &= 80.00 & R(p_0) &= 450.00 \\
p_1 &= 5 + 0.1 \cdot 80 = 13.00 & R'(p_1) &= 48.00 & R(p_1) &= 962.00 \\
p_2 &= 13 + 0.1 \cdot 48 = 17.80 & R'(p_2) &= 28.80 & R(p_2) &= 1146.32 \\
p_3 &= 17.80 + 0.1 \cdot 28.8 = 20.68 & R'(p_3) &= 17.28 & R(p_3) &= 1212.68 \\
p_4 &= 20.68 + 0.1 \cdot 17.28 = 22.41 & R'(p_4) &= 10.37 & R(p_4) &= 1236.56 \\
p_5 &= 22.41 + 0.1 \cdot 10.37 = 23.44 & R'(p_5) &= 6.22 & R(p_5) &= 1245.16
\end{aligned}
$$

Now the steps naturally get smaller as we approach the peak, allowing for both fast initial progress and precise convergence.

![Gradient ascent on R(p)](/images/posts/ml-is-optimization/quadratic-gradient-ascent.png)

Now let's move on to a more complex function.

## Optimizing a Single Variable Function

The previous example was a quadratic, where we could solve $R'(p) = 0$ analytically to find the critical point. But what if the derivative equation has no closed-form solution?

Consider $f(x) = x^4 - x^3 - x^2 + e^{-x^2}$. Setting the derivative to zero gives $4x^3 - 3x^2 - 2x - 2xe^{-x^2} = 0$, which we can't solve algebraically. This is where gradient descent becomes essential rather than just convenient.

![f(x) = x^4 - x^3 - x^2 + e^(-x^2)](/images/posts/ml-is-optimization/single-var-function.png)

This function has two local minima, one near $x = -0.62$ and a deeper one near $x = 1.24$. Since we're minimizing instead of maximizing, the update rule becomes $x_{n+1} = x_n - \alpha f'(x_n)$ (note the minus sign). Starting from $x_0 = 2.0$ with $\alpha = 0.1$:

$$
\begin{aligned}
x_0 &= 2.00 & f'(x_0) &= 15.93 & f(x_0) &= 4.02 \\
x_1 &= 2.00 - 0.1 \cdot 15.93 = 0.41 & f'(x_1) &= -1.73 & f(x_1) &= 0.64 \\
x_2 &= 0.41 - 0.1 \cdot (-1.73) = 0.58 & f'(x_2) &= -2.22 & f(x_2) &= 0.29 \\
x_3 &= 0.58 - 0.1 \cdot (-2.22) = 0.80 & f'(x_3) &= -2.31 & f(x_3) &= -0.22 \\
x_4 &= 0.80 - 0.1 \cdot (-2.31) = 1.03 & f'(x_4) &= -1.57 & f(x_4) &= -0.69
\end{aligned}
$$

After a few more steps, $x$ converges to the minimum at $x \approx 1.24$.

![Gradient descent from two starting points](/images/posts/ml-is-optimization/single-var-gradient-descent.png)

The plot shows two different starting points. Starting from $x_0 = 2.0$ leads to the deeper minimum on the right, while starting from $x_0 = -1.2$ leads to the shallower minimum on the left. The starting point determines which minimum we find, since gradient descent finds a local minimum, not necessarily the global one.

## Optimizing a Two Variable Function

Now let's add another variable. Consider $f(x, y) = x^2 + 3y^2 - xy$, which has a minimum at the origin.

![f(x, y) = x² + 3y² - xy](/images/posts/ml-is-optimization/two-var-function.png)

In the single variable case, we minimized $f(x)$ by changing $x$ in the direction that decreased $f$. With two variables, we can ask the same question twice: how should we change $x$ (holding $y$ fixed) to decrease $f$, and how should we change $y$ (holding $x$ fixed) to decrease $f$? A small step $(\Delta x, \Delta y)$ changes the function by the sum of the partial contributions:

$$
\Delta f \approx \frac{\partial f}{\partial x}\Delta x + \frac{\partial f}{\partial y}\Delta y
$$
Those two partial derivatives form a vector called the gradient:

$$
\nabla f = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right) = (2x - y, \; 6y - x)
$$

The gradient points in the direction of increasing $f$, so for small enough $\alpha$ we subtract it to descend. This is the same idea as the single variable update $x_{n+1} = x_n - \alpha f'(x_n)$, where we just replace the scalar derivative with a vector and take a step proportional to it in parameter space. The update rule is structurally identical to before:

$$
\mathbf{p} = (x, y), \qquad
\mathbf{p}_{n+1} = \mathbf{p}_n - \alpha \nabla f(\mathbf{p}_n)
$$

Starting from $(x_0, y_0) = (3, 2)$ with $\alpha = 0.1$:

$$
\begin{aligned}
(x_0, y_0) &= (3.00, 2.00) & \nabla f &= (4.00, 9.00) & f &= 15.00 \\
(x_1, y_1) &= (2.60, 1.10) & \nabla f &= (4.10, 4.00) & f &= 7.53 \\
(x_2, y_2) &= (2.19, 0.70) & \nabla f &= (3.68, 2.01) & f &= 4.73 \\
(x_3, y_3) &= (1.82, 0.50) & \nabla f &= (3.14, 1.18) & f &= 3.15 \\
(x_4, y_4) &= (1.51, 0.38) & \nabla f &= (2.64, 0.77) & f &= 2.14
\end{aligned}
$$

![Gradient descent on f(x, y)](/images/posts/ml-is-optimization/two-var-gradient-descent.png)

Notice the path doesn't head straight toward the minimum; it curves because the function is steeper in the $y$ direction. But the algorithm doesn't need to know this; it just follows the gradient downhill at each step.

Nothing fundamentally changed from the single variable case. We're still computing a derivative and stepping opposite to it. The only difference is that our derivative is now a vector.

In one dimension there are only two directions to move and only one decreases the function locally.
In two dimensions, there are many directions that decrease the function, so in principle there are many update rules we could use, such as changing one variable at a time.
So what's special about the gradient vector?

If we fix the length of a small update step $\Delta \mathbf{p}$, the change is $\Delta f \approx \nabla f \cdot \Delta \mathbf{p}$, and by [Cauchy-Schwarz](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality) this is most negative when $\Delta \mathbf{p}$ points opposite $\nabla f$.
That gives $\Delta \mathbf{p} = -\alpha \nabla f$, the steepest local decrease for a fixed step length.
Although we don't have to follow this direction, it explains why descending along the gradient is a principled default.

## Creating a Simple Model

Now that we have some tools, let's go back to the original problem of programmatically identifying whether an image shows a dog.
Mathematically, we want to find a function $f$ such that $f(\text{image of a dog}) = 1$ and $f(\text{anything else}) = 0$.

What kind of function could $f$ be? It could involve arbitrarily complex expressions, and we have no reason to expect it to take any particular form.
The [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) gives us a way forward. Under mild conditions, a neural network with enough parameters can approximate any function to arbitrary accuracy.
This reduces the problem from searching over all possible mathematical expressions to finding the right parameters $\boldsymbol{\theta}$ for a neural network $f_{\boldsymbol{\theta}}$.

Now we need to determine what parameters give us the function we want.
We already know how to minimize functions with gradient descent, so we need to turn "find parameters that make $f_{\boldsymbol{\theta}}$ accurate" into "minimize some function of $\boldsymbol{\theta}$."

The function $f_{\boldsymbol{\theta}}$ on its own has no notion of being minimized or maximized. But we can define a new function that measures how wrong $f_{\boldsymbol{\theta}}$ is.
Suppose we had access to every possible dog image and every possible non-dog image. We want $f_{\boldsymbol{\theta}}$ to output $1$ for each dog image and $0$ for everything else, so we can measure the total error as

$$
g(\boldsymbol{\theta}) = \sum_{\text{dog images}} \left(f_{\boldsymbol{\theta}}(\text{image}) - 1\right)^2 + \sum_{\text{other images}} \left(f_{\boldsymbol{\theta}}(\text{image}) - 0\right)^2
$$

Every term in this sum is zero when the prediction is correct and positive otherwise, so $g$ is minimized precisely when $f_{\boldsymbol{\theta}}$ correctly classifies every image.
This isn't the only function we could choose though.
Instead of squaring, we could raise it to the fourth power instead, or take the absolute value, and it would still be minimized when $f_{\boldsymbol{\theta}}$ correctly classifies every image.
Squaring is a nice choice because its derivative is smooth and easy to compute.

Of course, this function is uncomputable in practice since we can't enumerate every possible image. Instead, we collect a **training dataset** of $N$ labelled images, a representative sample of the full space. If we let $y_i$ denote the label of the $i$-th training image ($1$ for dog, $0$ otherwise), we replace the sum over all images with a sum over just our dataset:

$$
L(\boldsymbol{\theta}) = \sum_{i=1}^{N} \left(f_{\boldsymbol{\theta}}(\text{image}_i) - y_i\right)^2
$$

This is called the **loss function**. It takes in the neural network's parameters and returns a single number measuring how poorly the model performs on the training data. Minimizing it is structurally the same problem as the ones above. We compute $\nabla_{\boldsymbol{\theta}} L$, step opposite to it, and repeat. The parameters $\boldsymbol{\theta}$ play the same role as $p$ or $(x, y)$ did earlier; the only difference is that there may be millions of them instead of one or two.

The idea behind machine learning with neural networks follows directly from optimization.
We define a function that measures error, and minimize it with gradient descent. There's no new mathematical machinery beyond what we've already built up.

Of course, I've left off many of the details that make machine learning work well in practice. How well does a model generalize beyond the training data? What happens when the loss surface is nonconvex and riddled with local minima? How do we choose the network architecture, the optimizer, or the learning rate? These are the questions that the different subfields of machine learning try to answer, but the core loop remains the same. Define a loss, compute a gradient, and take a step.
