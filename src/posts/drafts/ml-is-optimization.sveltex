---
title: "Machine Learning Isn't Magic; It's Just Optimization"
date: "2026-01-30"
description: "Machine learning demystified: a visual tour of training as function minimization."
tags: ["machine-learning", "optimization"]
draft: true
---

<!-- TODO: intro - ML feels like magic, but strip away the jargon and it's
     mostly "find the input that makes a function as small as possible."
     Scope note: this post is about training/optimization, not the full ML
     pipeline (data collection, model choice, evaluation). Keep it visual. -->

## Minimizing a function

<!-- TODO: start with f(x) = (x - 3)^2, plot it, show the minimum is obvious
     visually. But what if we couldn't just solve f'(x) = 0 analytically?
     Seed the idea that in ML we call this f the "loss." -->

## Gradient descent

<!-- TODO: introduce the update rule x_{n+1} = x_n - Î± f'(x_n) on the same
     simple example. Walk through a few steps numerically.
     Show what happens with different learning rates:
     - too small: crawls toward the minimum
     - too large: overshoots, oscillates or diverges
     - just right: converges in a few steps
     Include plots for each case.
     Quick aside: some problems have closed-form solutions, but we use GD
     because it generalizes and is easy to visualize. Mention other optimizers
     exist without diving in. -->

## More than one variable

<!-- TODO: same idea but f(x, y), e.g. a quadratic bowl. Gradient is now a
     vector, update rule is the same. Maybe a contour plot with the descent
     path drawn on it. Key point: nothing conceptually changed, just more
     parameters. "Now imagine a million parameters." Tie x, y to "weights." -->

## Nonconvex landscapes

<!-- TODO: bumpy loss surfaces, local minima, saddle points, plateaus. The
     core algorithm doesn't change - you still walk downhill - but where you
     start can matter, and you might end up somewhere that isn't the global
     minimum. Brief, just acknowledge it. -->

## From data to a loss

<!-- TODO: bridge section. Start with a tiny dataset and a model family
     (e.g. lines with slope/intercept). Each parameter choice makes a
     prediction plot. Define a simple error measure (sum of squared errors)
     visually as vertical distances. This creates a loss surface in (m, b).
     Keep equations minimal; emphasize pictures. -->

## So what is machine learning, then?

<!-- TODO: concrete minimal example - linear regression on a tiny dataset.
     Show that "training" = "minimize this loss w.r.t. the weights." The
     weights are just the x (or x, y, ...) from before. The loss is just
     another f to minimize. Keep it visual: line fits, loss curve, and a
     descent path on the (m, b) surface. -->

<!-- TODO: closing - the magic is in choosing the model + loss (and data),
     not in the optimization itself. Optimization is the engine, not the
     whole car. -->
