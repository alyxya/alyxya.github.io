---
title: "Backpropagation"
date: "2026-02-17"
description: "Computing gradients through a neural network."
tags: ["machine-learning", "optimization", "calculus"]
---

The "gradient" in gradient descent is a vector of partial derivatives of the loss with respect to each parameter.
A partial derivative $\frac{\partial y}{\partial x}$ is just a ratio: how much $y$ changes when we change $x$ by a small amount.
If $\frac{\partial y}{\partial x} = 3$, then changing $x$ by $\epsilon$ causes $y$ to change by approximately $3\epsilon$, where this approximation is more accurate for smaller $\epsilon$.

What if we want $\frac{\partial z}{\partial x}$ but we only know intermediate partial derivatives $\frac{\partial z}{\partial y}$ and $\frac{\partial y}{\partial x}$?
The chain rule says we multiply them: $\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x}$.
If changing $x$ by $\epsilon_1$ causes $y$ to change by $3\epsilon_1$, and changing $y$ by $\epsilon_2$ causes $z$ to change by $5\epsilon_2$, then changing $x$ by $\epsilon_1$ causes $z$ to change by $15\epsilon_1$.
This can be seen by letting $\epsilon_2 = 3\epsilon_1$.
If $x$ affects $z$ through two intermediate variables $y_1$ and $y_2$, we add the contributions: $\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y_1} \cdot \frac{\partial y_1}{\partial x} + \frac{\partial z}{\partial y_2} \cdot \frac{\partial y_2}{\partial x}$.

A neural network is a long chain of differentiable operations, where the loss depends on each parameter through many intermediate layers.
Backpropagation is just the systematic application of the chain rule, working backwards from the loss through each operation to compute every partial derivative.
The most fundamental operation in a neural network is matrix multiplication, so let's start there.

## The Gradient of a Matrix Multiplication

Suppose $C = AB$ where $A$ is $2 \times 3$, $B$ is $3 \times 4$, and $C$ is $2 \times 4$:

$$
\begin{bmatrix} C_{11} & C_{12} & C_{13} & C_{14} \\ C_{21} & C_{22} & C_{23} & C_{24} \end{bmatrix} = \begin{bmatrix} A_{11} & A_{12} & A_{13} \\ A_{21} & A_{22} & A_{23} \end{bmatrix} \begin{bmatrix} B_{11} & B_{12} & B_{13} & B_{14} \\ B_{21} & B_{22} & B_{23} & B_{24} \\ B_{31} & B_{32} & B_{33} & B_{34} \end{bmatrix}
$$

Each element of $C$ is a dot product of a row of $A$ with a column of $B$:

$$
\begin{aligned}
C_{11} &= A_{11} B_{11} + A_{12} B_{21} + A_{13} B_{31} \\
C_{12} &= A_{11} B_{12} + A_{12} B_{22} + A_{13} B_{32} \\
C_{13} &= A_{11} B_{13} + A_{12} B_{23} + A_{13} B_{33} \\
C_{14} &= A_{11} B_{14} + A_{12} B_{24} + A_{13} B_{34} \\[6pt]
C_{21} &= A_{21} B_{11} + A_{22} B_{21} + A_{23} B_{31} \\
C_{22} &= A_{21} B_{12} + A_{22} B_{22} + A_{23} B_{32} \\
C_{23} &= A_{21} B_{13} + A_{22} B_{23} + A_{23} B_{33} \\
C_{24} &= A_{21} B_{14} + A_{22} B_{24} + A_{23} B_{34}
\end{aligned}
$$

From these equations we can read off partial derivatives directly. For example, $C_{11} = A_{11} B_{11} + A_{12} B_{21} + A_{13} B_{31}$, so $\frac{\partial C_{11}}{\partial A_{12}} = B_{21}$ and $\frac{\partial C_{11}}{\partial B_{21}} = A_{12}$. Each $C_{ij}$ depends only on row $i$ of $A$ and column $j$ of $B$, so most partial derivatives are zero: $\frac{\partial C_{11}}{\partial A_{21}} = 0$ since $A_{21}$ doesn't appear in $C_{11}$.

Now suppose $C$ feeds into some downstream computation that produces a scalar loss $L$. We're given $\frac{\partial L}{\partial C_{ij}}$ for every element of $C$, and want to find $\frac{\partial L}{\partial A_{ij}}$ and $\frac{\partial L}{\partial B_{ij}}$.

Consider $A_{12}$. It appears in $C_{11}$, $C_{12}$, $C_{13}$, and $C_{14}$, which is the entire first row of $C$. By the chain rule, we sum the effect through each of these:

$$
\frac{\partial L}{\partial A_{12}} = \frac{\partial L}{\partial C_{11}} \frac{\partial C_{11}}{\partial A_{12}} + \frac{\partial L}{\partial C_{12}} \frac{\partial C_{12}}{\partial A_{12}} + \frac{\partial L}{\partial C_{13}} \frac{\partial C_{13}}{\partial A_{12}} + \frac{\partial L}{\partial C_{14}} \frac{\partial C_{14}}{\partial A_{12}}
$$

Substituting the partial derivatives we computed above:

$$
\frac{\partial L}{\partial A_{12}} = \frac{\partial L}{\partial C_{11}} B_{21} + \frac{\partial L}{\partial C_{12}} B_{22} + \frac{\partial L}{\partial C_{13}} B_{23} + \frac{\partial L}{\partial C_{14}} B_{24}
$$

Writing out $\frac{\partial L}{\partial C}$ as a matrix:

$$
\frac{\partial L}{\partial C} = \begin{bmatrix} \frac{\partial L}{\partial C_{11}} & \frac{\partial L}{\partial C_{12}} & \frac{\partial L}{\partial C_{13}} & \frac{\partial L}{\partial C_{14}} \\ \frac{\partial L}{\partial C_{21}} & \frac{\partial L}{\partial C_{22}} & \frac{\partial L}{\partial C_{23}} & \frac{\partial L}{\partial C_{24}} \end{bmatrix}
$$

we see that $\frac{\partial L}{\partial A_{12}}$ is the dot product of row $1$ of $\frac{\partial L}{\partial C}$ with $\begin{bmatrix} B_{21} & B_{22} & B_{23} & B_{24} \end{bmatrix}$, which is row $2$ of $B$. In other words, it's the $(1, 2)$ entry of $\frac{\partial L}{\partial C} B^\top$.

The same pattern holds for every entry: $\frac{\partial L}{\partial A_{ij}}$ equals the dot product of row $i$ of $\frac{\partial L}{\partial C}$ with row $j$ of $B$. So the gradient of $A$ is itself a matrix multiplication:

$$
\frac{\partial L}{\partial A} = \frac{\partial L}{\partial C} B^\top
$$

Similarly, $B_{21}$ appears in $C_{11}$ and $C_{21}$, which is the entire first column of $C$, giving

$$
\frac{\partial L}{\partial B_{21}} = \frac{\partial L}{\partial C_{11}} A_{12} + \frac{\partial L}{\partial C_{21}} A_{22}
$$

which is the $(2, 1)$ entry of $A^\top \frac{\partial L}{\partial C}$. In general:

$$
\frac{\partial L}{\partial B} = A^\top \frac{\partial L}{\partial C}
$$

The dimensions confirm this: $\frac{\partial L}{\partial A}$ should be $2 \times 3$ like $A$, and $\frac{\partial L}{\partial C} B^\top$ is $(2 \times 4)(4 \times 3) = 2 \times 3$. Likewise $\frac{\partial L}{\partial B}$ should be $3 \times 4$ like $B$, and $A^\top \frac{\partial L}{\partial C}$ is $(3 \times 2)(2 \times 4) = 3 \times 4$.

To compute these gradients, the only information we needed from upstream was $\frac{\partial L}{\partial C}$: how much the loss changes per unit change in each element of $C$. We didn't need to know anything about what happens after $C$.

## A Small Neural Network

Now let's apply this to a neural network with $2$ inputs, $2$ hidden neurons with ReLU activations, and $1$ output:

$$
\begin{aligned}
\mathbf{z} &= W\mathbf{x} \\
\mathbf{h} &= \text{ReLU}(\mathbf{z}) \\
\hat{y} &= \mathbf{v}^\top \mathbf{h}
\end{aligned}
$$

The loss for a target $y$ is $L = (\hat{y} - y)^2$. Let's use concrete values:

$$
\mathbf{x} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad y = 2, \quad W = \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}, \quad \mathbf{v} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
$$

**Forward pass.**

$$
\mathbf{z} = \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 3 \\ -1 \end{bmatrix}, \quad \mathbf{h} = \text{ReLU}(\mathbf{z}) = \begin{bmatrix} 3 \\ 0 \end{bmatrix}
$$

$$
\hat{y} = \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 3 \\ 0 \end{bmatrix} = 3, \quad L = (3 - 2)^2 = 1
$$

Neuron $2$ has $z_2 = -1 < 0$, so ReLU zeros it out: $h_2 = 0$.

**Backward pass.** We work backwards from the loss. The loss derivative is:

$$
\frac{\partial L}{\partial \hat{y}} = 2(\hat{y} - y) = 2(1) = 2
$$

For the output layer, $\hat{y} = \mathbf{v}^\top \mathbf{h}$, so:

$$
\frac{\partial L}{\partial \mathbf{v}} = \frac{\partial L}{\partial \hat{y}} \mathbf{h} = 2 \begin{bmatrix} 3 \\ 0 \end{bmatrix} = \begin{bmatrix} 6 \\ 0 \end{bmatrix}
$$

To continue backwards, we need the gradient with respect to $\mathbf{h}$:

$$
\frac{\partial L}{\partial \mathbf{h}} = \frac{\partial L}{\partial \hat{y}} \mathbf{v} = 2 \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 2 \\ 2 \end{bmatrix}
$$

Through ReLU, we multiply elementwise by $\text{ReLU}'(\mathbf{z})$, which is $1$ where $z > 0$ and $0$ where $z < 0$:

$$
\frac{\partial L}{\partial \mathbf{z}} = \frac{\partial L}{\partial \mathbf{h}} \odot \text{ReLU}'(\mathbf{z}) = \begin{bmatrix} 2 \\ 2 \end{bmatrix} \odot \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 2 \\ 0 \end{bmatrix}
$$

The gradient through neuron $2$ is zero: the ReLU killed it in the forward pass, and now it kills the gradient in the backward pass.

For the hidden layer, $\mathbf{z} = W\mathbf{x}$ is a matrix multiplication. Applying the formula from the previous section:

$$
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \mathbf{z}} \, \mathbf{x}^\top = \begin{bmatrix} 2 \\ 0 \end{bmatrix} \begin{bmatrix} 1 & 2 \end{bmatrix} = \begin{bmatrix} 2 & 4 \\ 0 & 0 \end{bmatrix}
$$

The entire second row is zero because the dead ReLU blocks all gradient flow to neuron $2$'s weights.

**Numerical verification.** We claimed $\frac{\partial L}{\partial W_{11}} = 2$ and $\frac{\partial L}{\partial W_{12}} = 4$. Let's check both at once by setting $W_{11} = 1.001$ and $W_{12} = 1.001$:

$$
\begin{aligned}
z_1 &= 1.001(1) + 1.001(2) = 3.003 \\
\hat{y} &= 1(3.003) = 3.003 \\
L &= (3.003 - 2)^2 = 1.006009
\end{aligned}
$$

The predicted change is $\frac{\partial L}{\partial W_{11}}(0.001) + \frac{\partial L}{\partial W_{12}}(0.001) = 2(0.001) + 4(0.001) = 0.006$. The actual change is $1.006009 - 1 = 0.006009$, confirming that the partial derivatives correctly predict how the loss responds to parameter changes.

**Gradient descent.** Using a learning rate of $\alpha = 0.05$, we update $W$ and $\mathbf{v}$ by subtracting $\alpha$ times their gradients:

$$
W = \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} - 0.05 \begin{bmatrix} 2 & 4 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 0.9 & 0.8 \\ 1 & -1 \end{bmatrix}, \quad \mathbf{v} = \begin{bmatrix} 1 \\ 1 \end{bmatrix} - 0.05 \begin{bmatrix} 6 \\ 0 \end{bmatrix} = \begin{bmatrix} 0.7 \\ 1 \end{bmatrix}
$$

Running the forward pass with the updated parameters:

$$
\mathbf{z} = \begin{bmatrix} 0.9 & 0.8 \\ 1 & -1 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 2.5 \\ -1 \end{bmatrix}, \quad \mathbf{h} = \begin{bmatrix} 2.5 \\ 0 \end{bmatrix}, \quad \hat{y} = 0.7(2.5) = 1.75, \quad L = (1.75 - 2)^2 = 0.0625
$$

The loss dropped from $1$ to $0.0625$, a $94\%$ reduction in a single step.
