---
title: "Backpropagation"
date: "2026-02-17"
description: "Computing gradients through a neural network."
tags: ["machine-learning", "optimization", "calculus"]
---

In [a previous post](/blog/ml-is-optimization), we defined the loss function as a measure of how wrong a model's predictions are, and minimized it with gradient descent. But we glossed over a key step: how do we actually compute the gradient of the loss with respect to every parameter in a neural network? That's what backpropagation answers.

The "gradient" in gradient descent is a vector of partial derivatives of the loss with respect to each parameter.
A partial derivative $\frac{\partial y}{\partial x}$ is just a ratio: how much $y$ changes when we change $x$ by a small amount.
If $\frac{\partial y}{\partial x} = 3$, then changing $x$ by $\epsilon$ causes $y$ to change by approximately $3\epsilon$, where this approximation is more accurate for smaller $\epsilon$.

What if we want $\frac{\partial z}{\partial x}$ but we only know intermediate partial derivatives $\frac{\partial z}{\partial y}$ and $\frac{\partial y}{\partial x}$?
The chain rule says we multiply them: $\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x}$.
If changing $x$ by $\epsilon_1$ causes $y$ to change by $3\epsilon_1$, and changing $y$ by $\epsilon_2$ causes $z$ to change by $5\epsilon_2$, then changing $x$ by $\epsilon_1$ causes $z$ to change by $15\epsilon_1$.
This can be seen by letting $\epsilon_2 = 3\epsilon_1$.
If $x$ affects $z$ through two intermediate variables $y_1$ and $y_2$, we add the contributions: $\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y_1} \cdot \frac{\partial y_1}{\partial x} + \frac{\partial z}{\partial y_2} \cdot \frac{\partial y_2}{\partial x}$.

A neural network is a long chain of differentiable operations, where the loss depends on each parameter through many intermediate layers.
Backpropagation is just the systematic application of the chain rule, working backwards from the loss through each operation to compute every partial derivative.
The most fundamental operation in a neural network is matrix multiplication, so let's start there.

## The Gradient of a Matrix Multiplication

Suppose $C = AB$ where $A$ is $2 \times 3$, $B$ is $3 \times 4$, and $C$ is $2 \times 4$:

$$
\begin{bmatrix} C_{11} & C_{12} & C_{13} & C_{14} \\ C_{21} & C_{22} & C_{23} & C_{24} \end{bmatrix} = \begin{bmatrix} A_{11} & A_{12} & A_{13} \\ A_{21} & A_{22} & A_{23} \end{bmatrix} \begin{bmatrix} B_{11} & B_{12} & B_{13} & B_{14} \\ B_{21} & B_{22} & B_{23} & B_{24} \\ B_{31} & B_{32} & B_{33} & B_{34} \end{bmatrix}
$$

Each element of $C$ is a dot product of a row of $A$ with a column of $B$:

$$
\begin{aligned}
C_{11} &= A_{11} B_{11} + A_{12} B_{21} + A_{13} B_{31} \\
C_{12} &= A_{11} B_{12} + A_{12} B_{22} + A_{13} B_{32} \\
C_{13} &= A_{11} B_{13} + A_{12} B_{23} + A_{13} B_{33} \\
C_{14} &= A_{11} B_{14} + A_{12} B_{24} + A_{13} B_{34} \\[6pt]
C_{21} &= A_{21} B_{11} + A_{22} B_{21} + A_{23} B_{31} \\
C_{22} &= A_{21} B_{12} + A_{22} B_{22} + A_{23} B_{32} \\
C_{23} &= A_{21} B_{13} + A_{22} B_{23} + A_{23} B_{33} \\
C_{24} &= A_{21} B_{14} + A_{22} B_{24} + A_{23} B_{34}
\end{aligned}
$$

From these equations we can read off partial derivatives directly. For example, $C_{11} = A_{11} B_{11} + A_{12} B_{21} + A_{13} B_{31}$, so $\frac{\partial C_{11}}{\partial A_{12}} = B_{21}$ and $\frac{\partial C_{11}}{\partial B_{21}} = A_{12}$. Each $C_{ij}$ depends only on row $i$ of $A$ and column $j$ of $B$, so most partial derivatives are zero: $\frac{\partial C_{11}}{\partial A_{21}} = 0$ since $A_{21}$ doesn't appear in $C_{11}$.

Now suppose $C$ feeds into some downstream computation that produces a scalar loss $L$. We're given $\frac{\partial L}{\partial C_{ij}}$ for every element of $C$, and want to find $\frac{\partial L}{\partial A_{ij}}$ and $\frac{\partial L}{\partial B_{ij}}$.

Consider $A_{12}$. It appears in $C_{11}$, $C_{12}$, $C_{13}$, and $C_{14}$, which is the entire first row of $C$. By the chain rule, we sum the effect through each of these:

$$
\frac{\partial L}{\partial A_{12}} = \frac{\partial L}{\partial C_{11}} \frac{\partial C_{11}}{\partial A_{12}} + \frac{\partial L}{\partial C_{12}} \frac{\partial C_{12}}{\partial A_{12}} + \frac{\partial L}{\partial C_{13}} \frac{\partial C_{13}}{\partial A_{12}} + \frac{\partial L}{\partial C_{14}} \frac{\partial C_{14}}{\partial A_{12}}
$$

Substituting the partial derivatives we computed above:

$$
\frac{\partial L}{\partial A_{12}} = \frac{\partial L}{\partial C_{11}} B_{21} + \frac{\partial L}{\partial C_{12}} B_{22} + \frac{\partial L}{\partial C_{13}} B_{23} + \frac{\partial L}{\partial C_{14}} B_{24}
$$

Writing out $\frac{\partial L}{\partial C}$ as a matrix:

$$
\frac{\partial L}{\partial C} = \begin{bmatrix} \frac{\partial L}{\partial C_{11}} & \frac{\partial L}{\partial C_{12}} & \frac{\partial L}{\partial C_{13}} & \frac{\partial L}{\partial C_{14}} \\ \frac{\partial L}{\partial C_{21}} & \frac{\partial L}{\partial C_{22}} & \frac{\partial L}{\partial C_{23}} & \frac{\partial L}{\partial C_{24}} \end{bmatrix}
$$

we see that $\frac{\partial L}{\partial A_{12}}$ is the dot product of row $1$ of $\frac{\partial L}{\partial C}$ with $\begin{bmatrix} B_{21} & B_{22} & B_{23} & B_{24} \end{bmatrix}$, which is row $2$ of $B$. In other words, it's the $(1, 2)$ entry of $\frac{\partial L}{\partial C} B^\top$.

The same pattern holds for every entry: $\frac{\partial L}{\partial A_{ij}}$ equals the dot product of row $i$ of $\frac{\partial L}{\partial C}$ with row $j$ of $B$. So the gradient of $A$ is itself a matrix multiplication:

$$
\frac{\partial L}{\partial A} = \frac{\partial L}{\partial C} B^\top
$$

Similarly, $B_{21}$ appears in $C_{11}$ and $C_{21}$, which is the entire first column of $C$, giving

$$
\frac{\partial L}{\partial B_{21}} = \frac{\partial L}{\partial C_{11}} A_{12} + \frac{\partial L}{\partial C_{21}} A_{22}
$$

which is the $(2, 1)$ entry of $A^\top \frac{\partial L}{\partial C}$. In general:

$$
\frac{\partial L}{\partial B} = A^\top \frac{\partial L}{\partial C}
$$

The dimensions confirm this: $\frac{\partial L}{\partial A}$ should be $2 \times 3$ like $A$, and $\frac{\partial L}{\partial C} B^\top$ is $(2 \times 4)(4 \times 3) = 2 \times 3$. Likewise $\frac{\partial L}{\partial B}$ should be $3 \times 4$ like $B$, and $A^\top \frac{\partial L}{\partial C}$ is $(3 \times 2)(2 \times 4) = 3 \times 4$.

To compute these gradients, the only information we needed from upstream was $\frac{\partial L}{\partial C}$: how much the loss changes per unit change in each element of $C$. We didn't need to know anything about what happens after $C$.

## A Small Neural Network

Now let's apply this to a neural network with $2$ inputs, $2$ hidden neurons with ReLU activations ($\text{ReLU}(z) = \max(0, z)$), and $1$ output:

$$
\begin{aligned}
\mathbf{z} &= W\mathbf{x} \\
\mathbf{h} &= \text{ReLU}(\mathbf{z}) \\
\hat{y} &= \mathbf{v}^\top \mathbf{h}
\end{aligned}
$$

The loss for a target $y$ is $L = (\hat{y} - y)^2$. Let's use concrete values:

$$
\mathbf{x} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad y = 2, \quad W = \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}, \quad \mathbf{v} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
$$

The forward pass computes each layer in sequence:

$$
\mathbf{z} = \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 3 \\ -1 \end{bmatrix}, \quad \mathbf{h} = \text{ReLU}(\mathbf{z}) = \begin{bmatrix} 3 \\ 0 \end{bmatrix}
$$

$$
\hat{y} = \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 3 \\ 0 \end{bmatrix} = 3, \quad L = (3 - 2)^2 = 1
$$

Now we work backwards from the loss, applying the chain rule through each operation:

$$
\frac{\partial L}{\partial \hat{y}} = 2(\hat{y} - y) = 2
$$

Since $\hat{y} = \mathbf{v}^\top \mathbf{h}$, the gradient flows to both $\mathbf{v}$ and $\mathbf{h}$:

$$
\frac{\partial L}{\partial \mathbf{v}} = \frac{\partial L}{\partial \hat{y}} \mathbf{h} = 2 \begin{bmatrix} 3 \\ 0 \end{bmatrix} = \begin{bmatrix} 6 \\ 0 \end{bmatrix}, \qquad \frac{\partial L}{\partial \mathbf{h}} = \frac{\partial L}{\partial \hat{y}} \mathbf{v} = 2 \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 2 \\ 2 \end{bmatrix}
$$

Through the ReLU, whose derivative is $1$ where $z > 0$ and $0$ where $z \leq 0$:

$$
\frac{\partial L}{\partial \mathbf{z}} = \frac{\partial L}{\partial \mathbf{h}} \odot \text{ReLU}'(\mathbf{z}) = \begin{bmatrix} 2 \\ 2 \end{bmatrix} \odot \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 2 \\ 0 \end{bmatrix}
$$

Finally, $\mathbf{z} = W\mathbf{x}$ is a matrix multiplication, so applying the formula from the previous section:

$$
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \mathbf{z}} \, \mathbf{x}^\top = \begin{bmatrix} 2 \\ 0 \end{bmatrix} \begin{bmatrix} 1 & 2 \end{bmatrix} = \begin{bmatrix} 2 & 4 \\ 0 & 0 \end{bmatrix}
$$

The entire second row is zero because neuron $2$ had $z_2 = -1 < 0$: the ReLU zeroed it out in the forward pass, and now zeros out its gradient in the backward pass.

The parameters of this network are $W$ and $\mathbf{v}$, and we computed their gradients:

$$
\frac{\partial L}{\partial W} = \begin{bmatrix} 2 & 4 \\ 0 & 0 \end{bmatrix}, \quad \frac{\partial L}{\partial \mathbf{v}} = \begin{bmatrix} 6 \\ 0 \end{bmatrix}
$$

Each gradient tells us how the loss changes when we adjust that parameter. To verify them all at once, let's increase every parameter by $\epsilon = 0.001$. The predicted change in loss is $\epsilon$ times the sum of all gradient entries:

$$
\Delta L \approx (2 + 4 + 0 + 0 + 6 + 0) \cdot 0.001 = 0.012
$$

Rerunning the forward pass with $W + \epsilon$ and $\mathbf{v} + \epsilon$:

$$
\mathbf{z} = \begin{bmatrix} 1.001 & 1.001 \\ 1.001 & -0.999 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 3.003 \\ -0.997 \end{bmatrix}, \quad \mathbf{h} = \begin{bmatrix} 3.003 \\ 0 \end{bmatrix}
$$

$$
\hat{y} = \begin{bmatrix} 1.001 & 1.001 \end{bmatrix} \begin{bmatrix} 3.003 \\ 0 \end{bmatrix} = 3.006003, \quad L = (3.006003 - 2)^2 \approx 1.012042
$$

The actual change in $L$ is roughly $1.012042 - 1 = 0.012042$, closely matching the predicted $0.012$.

In practice, we use these gradients to improve the model through gradient descent: each parameter is updated by subtracting $\alpha$ times its gradient, where $\alpha$ is a learning rate. With $\alpha = 0.001$:

$$
W = \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} - 0.001 \begin{bmatrix} 2 & 4 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 0.998 & 0.996 \\ 1 & -1 \end{bmatrix}, \quad \mathbf{v} = \begin{bmatrix} 1 \\ 1 \end{bmatrix} - 0.001 \begin{bmatrix} 6 \\ 0 \end{bmatrix} = \begin{bmatrix} 0.994 \\ 1 \end{bmatrix}
$$

The predicted change in loss sums each parameter's contribution:

$$
\frac{\partial L}{\partial W_{11}} \Delta W_{11} + \frac{\partial L}{\partial W_{12}} \Delta W_{12} + \frac{\partial L}{\partial v_1} \Delta v_1 = 2(-0.002) + 4(-0.004) + 6(-0.006) = -0.056
$$

Rerunning the forward pass with the updated parameters:

$$
\mathbf{z} = \begin{bmatrix} 0.998 & 0.996 \\ 1 & -1 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 2.990 \\ -1 \end{bmatrix}, \quad \mathbf{h} = \begin{bmatrix} 2.990 \\ 0 \end{bmatrix}
$$

$$
\hat{y} = \begin{bmatrix} 0.994 & 1 \end{bmatrix} \begin{bmatrix} 2.990 \\ 0 \end{bmatrix} = 2.97206, \quad L = (2.97206 - 2)^2 \approx 0.9449
$$

The actual change is roughly $0.9449 - 1 = -0.0551$, closely matching the predicted $-0.056$.
