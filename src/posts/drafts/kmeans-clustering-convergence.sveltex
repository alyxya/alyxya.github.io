---
title: "K-Means Clustering Convergence"
date: "2026-02-06"
description: "K-means minimizes a function by alternating two local improvements."
tags: ["machine-learning", "optimization", "clustering"]
---

Suppose you have unlabelled data points and want to categorize them in some way.
Since you don't have labels, it's unclear how the data should be categorized.
One simple way is to categorize data into clusters where data points near each other form a cluster.

So how do we go about this?
In the [previous post](/blog/ml-is-optimization), we minimized functions using gradient descent, where the fundamental principle behind the gradient update is to take a small step that locally minimizes the function.
Naturally, it makes sense to do something similar here: construct a function where minimizing it forms good clusters, and then minimize it.
This is what k-means clustering does.

The function k-means minimizes is the total squared distance from each point to its assigned cluster center:

$$
J = \sum_{i=1}^{n} \|\mathbf{x}_i - \mu_{c_i}\|^2
$$

Unlike neural networks with continuous parameters that we can locally minimize with gradient descent, this function involves discrete cluster assignments that aren't differentiable.
Therefore, the procedure here needs to take into consideration how the function can be locally minimized.
There are two kinds of local moves: change a point's assigned cluster, or change the center of some cluster.
The former is a discrete change, while the latter is a continuous change where gradient descent can be used.

The best local change to a point's assignment is to reassign it to whichever cluster has its center closest.
This extends to instead of just reassigning one point, we might as well reassign all of them.

The best local change for the center of a cluster would ordinarily be a gradient descent step, but since the function is a simple quadratic polynomial, we can just set the center directly to the minimum.
This turns out to be the centroid or average of all points assigned to the corresponding cluster.

note: maybe sketch proof

Thus, the k-means clustering procedure boils down to alternating between these two local changes until no local change will decrease the function anymore.
This procedure necessarily converges because the function is nonnegative, each step decreases it, and there are finitely many possible assignments.

To conclude, k-means clustering isn't some arbitrary procedure. It follows the same pattern as gradient descent, where we define an objective, then iteratively decrease it by making local improvements.
