---
title: "K-Means Clustering Convergence"
date: "2026-02-06"
description: "K-means minimizes a function by alternating two local improvements."
tags: ["machine-learning", "optimization", "clustering"]
---

Suppose you have unlabelled data points and want to categorize them in some way.
Since you don't have labels, it's unclear how the data should be categorized.
One simple way is to categorize data into clusters where data points near each other form a cluster.

So how do we go about this?
In the [previous post](/blog/ml-is-optimization), we minimized functions using gradient descent, where the fundamental principle behind the gradient update is to take a small step that locally minimizes the function.
Naturally, it makes sense to do something similar here: construct a function where minimizing it forms good clusters, and then minimize it.
This is what k-means clustering does.

The function k-means minimizes is the total squared distance from each point to its assigned cluster center:

$$
J = \sum_{i=1}^{n} \|\mathbf{x}_i - \mu_{c_i}\|^2
$$

Unlike neural networks with continuous parameters that we can locally minimize with gradient descent, this function is discrete in having assigned clusters be non-differentiable.
Therefore, the procedure here needs to take into consideration how the function can be locally minimized.
The smallest units of change for the function are to either change the assigned cluster for a data point, or to change the center of the cluster.
The former is a discrete change, while the latter is a continuous change where gradient descent can be used.

The best local change for the assigned cluster for a data point is to reassign its cluster to whichever cluster has its center closest to the data point.
This can be extended where instead of just reassigning the cluster for a single data point, we might as well reassign the clusters for all data points.

The best local change for the center of a cluster would ordinarily be to perform a gradient descent step, but since the function is just a simple quadratic polynomial with a directly computable minimum, we can just reassign the center directly to the minimum.
This turns out to be the centroid or average of all points assigned to the corresponding cluster.

note: maybe sketch proof

Thus, the k-means clustering procedure boils down to alternating between these two local changes until no local change will decrease the function anymore.
This procedure necessarily converges at some point because the function is nonnegative and each step decreases it.

To conclude, k-means clustering isn't some arbitrary procedure, rather it follows the same pattern as gradient descent: define an objective, then iteratively decrease it by making local improvements.
