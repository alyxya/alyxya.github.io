---
title: "Almost Orthogonal Vectors"
date: "2026-01-29"
description: ""
tags: ["linear-algebra", "vectors"]
draft: true
---

A neural network represents features as directions in its activation space.
Ideally, each feature gets its own orthogonal direction.
This way, features don't interfere with each other, since the dot product between any two orthogonal vectors is zero, and then you can always decompose any vector as the weighted sum of the feature basis vectors.
But orthogonality means in $n$ dimensions, you can have at most $n$ mutually orthogonal feature vectors.

A really interesting phenomenon is the notion of [superposition](https://transformer-circuits.pub/2022/toy_model/index.html), where models represent more features than they have dimensions by allowing all features to be almost orthogonal to each other.
This introduces a small amount of interference between features and there's no longer a unique feature decomposition for every vector, but models can express far more features as a result.
I figured it would be worthwhile to try approximating just how many features can be represented this way in $n$ dimensions.

---

More precisely: given $n$ dimensions and a small $\epsilon > 0$, what is the maximum number of unit vectors we can find such that every pair has dot product at most $\epsilon$ in absolute value? Recall that the [dot product between unit vectors equals the cosine of the angle between them](/blog/dot-products), and $\cos 90^\circ = 0$, so we're asking how many vectors can be *almost* orthogonal to each other.

Formalizing the problem, given $n \in \mathbb{N}$ and $\epsilon > 0$, approximate the maximum size of a set $S \subset \mathbb{R}^n$ such that:
1. $\|v\| = 1$ for all $v \in S$, and
2. $|u \cdot v| \leq \epsilon$ for all distinct $u, v \in S$.

Note that for $\epsilon = 0$ we recover ordinary orthogonality and $|S| \leq n$.

We'll show that $|S|$ can be exponential in $n$ using a probabilistic argument. The idea is to build $S$ greedily: pick vectors one at a time, and show that at each step a random vector is very likely to be almost orthogonal to all previously chosen vectors.

To sample a uniformly random direction in $\mathbb{R}^n$, draw each coordinate independently from the standard normal distribution $N(0, 1)$ and normalize the result. This works because the standard normal distribution is spherically symmetric, where the probability density $\prod_i e^{-x_i^2/2}$ depends only on $\|x\|^2$, so all directions are equally likely.

Suppose we fix a unit vector $v$ and sample a random unit vector $w$. Without loss of generality, let $v = e_1 = (1, 0, \ldots, 0)$, so that $v \cdot w = w_1$, the first coordinate of $w$. If $g = (g_1, \ldots, g_n)$ is our unnormalized Gaussian vector, then

$$
v \cdot w = \frac{g_1}{\|g\|} = \frac{g_1}{\sqrt{g_1^2 + \cdots + g_n^2}}.
$$

Since each $g_i^2$ has mean $1$, by the law of large numbers $\|g\|^2 \approx n$ for large $n$, so $v \cdot w \approx g_1/\sqrt{n}$. Since $g_1 \sim N(0,1)$, the dot product $v \cdot w$ behaves like a $N(0, 1/n)$ random variable, and the probability that $|v \cdot w|$ exceeds $\epsilon$ is approximately

$$
P\!\left(\left|\frac{g_1}{\sqrt{n}}\right| > \epsilon\right) = P(|g_1| > \epsilon\sqrt{n}).
$$

The standard Gaussian tail bound gives $P(|Z| > t) \leq 2e^{-t^2/2}$ for $Z \sim N(0,1)$, so

$$
P(|v \cdot w| > \epsilon) \lessapprox 2e^{-n\epsilon^2/2}.
$$

Now we build $S$ one vector at a time. Suppose we've already chosen $v_1, \ldots, v_{k-1}$, and we want to add a $k$-th vector. For each existing $v_i$, the "forbidden region" — where a random unit vector $w$ would have $|v_i \cdot w| > \epsilon$ — has probability at most $2e^{-n\epsilon^2/2}$.

By the union bound, the probability that $w$ falls in *any* forbidden region is at most

$$
(k - 1) \cdot 2e^{-n\epsilon^2/2}.
$$

As long as this is less than $1$, there exists a valid choice for $v_k$. This holds whenever

$$
k - 1 < \frac{1}{2} e^{n\epsilon^2/2},
$$

so we can greedily build a set of at least

$$
|S| \geq \left\lceil \frac{1}{2} e^{n\epsilon^2/2} \right\rceil
$$

mutually $\epsilon$-almost-orthogonal unit vectors. This is exponential in $n$ for any fixed $\epsilon > 0$ despite being a very weak lower bound.

---

TODO: show empirical data and charts from experiments with sampling random vectors
